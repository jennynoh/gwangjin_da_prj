{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f0d34a-753b-41b4-b405-108feeadf09c",
   "metadata": {},
   "source": [
    "# Selenium으로 네이버 뉴스 스크랩\n",
    "- 검색필터: 관련도 순"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a360169-1813-4f61-8466-45496f19175f",
   "metadata": {},
   "source": [
    "## version 1. Selenium으로 직접 조작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f258d468-4e4b-4151-aec4-be43489b06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Chrome WebDriver 설정\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=webdriver.ChromeOptions())\n",
    "\n",
    "driver.get('https://www.naver.com')\n",
    "\n",
    "ele = driver.find_element(By.ID, 'query')\n",
    "ele.send_keys('광진구')\n",
    "ele.send_keys(Keys.ENTER)\n",
    "\n",
    "news = driver.find_element(by=By.LINK_TEXT, value='뉴스')\n",
    "news.click()\n",
    "\n",
    "# 날짜 필터 걸기\n",
    "driver.find_element(by=By.LINK_TEXT, value='옵션').click()\n",
    "driver.find_element(by=By.LINK_TEXT, value='직접입력').click()\n",
    "## selector scroll "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec772edb-4242-441d-8e85-b161fea73c7f",
   "metadata": {},
   "source": [
    "## version 2. url 값 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5512ac8-1fb9-4ecb-a29b-23bafe9f2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import pandas as pd \n",
    "\n",
    "def main():\n",
    "    # 검색필터 설정\n",
    "    keyword = input('검색어를 입력해주세요.')\n",
    "    start_date = input('시작 날짜를 입력해주세요(예: 2023.01.01)')\n",
    "    end_date = input('마지막 날짜를 입력해주세요(예: 2023.12.31)')\n",
    "    \n",
    "    # url 접속\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=webdriver.ChromeOptions())\n",
    "    url = f'https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={start_date}&de={end_date}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20230101to20240319&is_sug_officeid=0&office_category=0&service_area=0'\n",
    "    driver.get(url)\n",
    "\n",
    "    # 스크롤\n",
    "    last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    while True:\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "        time.sleep(1)\n",
    "        new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    \n",
    "        if last_height != new_height:\n",
    "            last_height = new_height\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # 기사제목 & url 가져오기 \n",
    "    titles = []\n",
    "    urls = []\n",
    "    pub_dates = []\n",
    "    press = []\n",
    "\n",
    "    bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    articles = bs.findAll('div', attrs={'class':'news_area'})\n",
    "    for article in articles:\n",
    "        titles.append(article.find('a', attrs={'class':'news_tit'}).text)\n",
    "        urls.append(article.find('a', attrs={'class':'news_tit'})['href'])\n",
    "        pub_dates.append(article.find('span', attrs={'class':'info'}).text)\n",
    "        press.append(article.find('a', attrs={'class':'info press'}).text)\n",
    "\n",
    "    contents = []\n",
    "    comments = []\n",
    "    # # 기사내용 & 댓글 가져오기 \n",
    "    for url in urls:\n",
    "        try:\n",
    "            driver.implicitly_wait(1)\n",
    "            driver.get(url)\n",
    "            bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            try:\n",
    "                contents.append(bs.find('div', attrs={'itemprop':'articleBody'}).text)\n",
    "            except:\n",
    "                contents.append('')\n",
    "        except TimeoutException:\n",
    "            contents.append('')\n",
    "            print(\"페이지 로딩 시간이 초과되었습니다.\")\n",
    "            continue\n",
    "   \n",
    "            \n",
    "\n",
    "    # cvs로 반환\n",
    "    df = pd.DataFrame({'press':press, 'title':titles, 'pub_date':pub_dates, 'url':urls, 'contents':contents})\n",
    "    df.to_csv(f'{keyword}_news_{start_date}_{end_date}.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d428f195-35ed-441f-80c5-afca8e7bfbcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어를 입력해주세요. 광진구\n",
      "시작 날짜를 입력해주세요(예: 2023.01.01) 2023.01.01\n",
      "마지막 날짜를 입력해주세요(예: 2023.12.31) 2023.01.01\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "WebDriver.implicitly_wait() missing 1 required positional argument: 'time_to_wait'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[67], line 55\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m         driver\u001b[38;5;241m.\u001b[39mimplicitly_wait()\n\u001b[1;32m     56\u001b[0m         driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     57\u001b[0m         bs \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: WebDriver.implicitly_wait() missing 1 required positional argument: 'time_to_wait'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c56f8-4e14-4b33-98e8-50e2c11b5955",
   "metadata": {},
   "source": [
    "## version 3. 메인 언론사 기사만 가져오기\n",
    "- 연합뉴스, 연합뉴스(앱), 매일경제, 매일경제(미라클), MBC, SBS, KBS, JTBC\n",
    "- 참고: https://everyday-tech.tistory.com/entry/3%ED%83%84-%EC%89%BD%EA%B2%8C-%EB%94%B0%EB%9D%BC%ED%95%98%EB%8A%94-%EB%84%A4%EC%9D%B4%EB%B2%84-%EB%89%B4%EC%8A%A4-%ED%81%AC%EB%A1%A4%EB%A7%81-%EB%B3%B8%EB%AC%B8-%EA%B0%80%EC%A0%B8%EC%98%A4%EA%B8%B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a2806-0f42-481b-ace0-70b07b358fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "def main():\n",
    "    # 검색필터 설정\n",
    "    keyword = input('검색어를 입력해주세요.')\n",
    "    start_date = input('시작 날짜를 입력해주세요(예: 2023.01.01)')\n",
    "    end_date = input('마지막 날짜를 입력해주세요(예: 2023.12.31)')\n",
    "    \n",
    "    # url 접속\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=webdriver.ChromeOptions())\n",
    "    url = f'https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={start_date}&de={end_date}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20230101to20240319&is_sug_officeid=0&office_category=0&service_area=0'\n",
    "    driver.get(url)\n",
    "\n",
    "    # 스크롤\n",
    "    last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    while True:\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "        time.sleep(1)\n",
    "        new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    \n",
    "        if last_height != new_height:\n",
    "            last_height = new_height\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # 기사제목 & url 가져오기 \n",
    "    titles = []\n",
    "    urls = []\n",
    "    pub_dates = []\n",
    "    press = []\n",
    "\n",
    "    bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    articles = bs.findAll('div', attrs={'class':'news_area'})\n",
    "    for article in articles:\n",
    "        titles.append(article.find('a', attrs={'class':'news_tit'}).text)\n",
    "        urls.append(article.find('a', attrs={'class':'news_tit'})['href'])\n",
    "        pub_dates.append(article.find('span', attrs={'class':'info'}).text)\n",
    "        press.append(article.find('a', attrs={'class':'info press'}).text)\n",
    "\n",
    "    contents = []\n",
    "    comments = []\n",
    "    # # 기사내용 & 댓글 가져오기 \n",
    "    for url in urls:\n",
    "        time.sleep(3)\n",
    "        driver.get(url)\n",
    "        bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        try:\n",
    "            contents.append(bs.find('div', attrs={'itemprop':'articleBody'}).text)\n",
    "        except:\n",
    "            contents.append('')\n",
    "            \n",
    "\n",
    "    # cvs로 반환\n",
    "    df = pd.DataFrame({'press':press, 'title':titles, 'pub_date':pub_dates, 'url':urls, 'contents':contents})\n",
    "    df.to_csv(f'{keyword}_news_{start_date}_{end_date}.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
